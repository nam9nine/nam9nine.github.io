---
title: "[밑바닥부터 시작하는 딥러닝1] Chapter2 퍼셉트론"
date: 2025-01-04 00:00:00 +0900
category: [AI, 밑바닥부터 시작하는 딥러닝1]
tags: [deep learning, study]
math: true
comments: true
mermaid: true
image:
  path: /assets/img/posts/AI/underbook/main.png
  lqip: data:image/png;base64,....
---

> 퍼셉트론의 기원에 대한 설명은 이 포스트에서 생략하고, 다음 챕터에서 다루겠습니다. 여기서는 개념 자체에 초점을 맞출 것입니다.
{: .prompt-warning}

## 퍼셉트론이란?
> `다수의 신호`를 입력 받아 `하나의 신호`를 출력하는 것

- 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 고안한 알고리즘
- 신경망(딥러닝)의 기원이 되는 알고리즘

## 구성요소

> 다음은 퍼셉트론을 간단하게 시각화한 사진과 아래 표를 통해 퍼셉트론의 구성요소들을 살펴보겠습니다.

![alt](/assets/img/posts/AI/underbook/chap2/1.png){:w="400"}
_퍼셉트론 시각화_

|요소                |비유       |
|$$x$$(입력 신호)    | 여러 개의 스위치(입력 신호)|
|$$y$$(출력 신호)    |  전구의 최종 상태(켜짐 or 꺼짐)|
|$$w$$(가중치 )      |각 스위치에 연결된 조광기(가중치)   |
|$$\theta$$(임계값)  |  전구가 켜지기 위한 최소 전류의 세기|
|원 도형(노드, 뉴런) | 전구|

## 퍼셉트론 출력 결정
입력 신호들이 뉴런에 신호를 보낼 때 입력 신호값과 가중치가 각각 곱해집니다($$x_1w_1$$, $$x_2w_2$$)

이렇게 곱해진 값들을 모두 더한 게 임계값을 넘게 되면 출력 신호를 1, 넘지 못 하게 되면 0으로 정해집니다.

수식으로 나타내면 다음과 같습니다.

$$
y =
\begin{cases}
0 & \text{if } w_1x_1 + w_2x_2 \leq \theta \\
1 & \text{if } w_1x_1 + w_2x_2 > \theta
\end{cases}
$$

> 이전 설명을 비유를 통해 설명을 해보겠습니다.

전구를 단순히 켤 수 있는 A 스위치 ($$x_1$$)와 B 스위치 ($$x_2$$)가 있습니다. A 스위치를 켜고 ($$x_1 = 1$$), B 스위치는 끕니다 ($$x_2 = 0$$)

각 스위치는 조광기(가중치 $$w_1$$, $$w_2$$)를 통해 전류의 세기를 조절합니다. A 스위치에서 오는 전류는 조광기를 만나 $$x_1 \times w_1$$ 만큼의 세기로 증가하고, B 스위치는 꺼져 있어서 전류가 흐르지 않습니다 ($$x_2 \times w_2 = 0$$)

이렇게 전구에 흐르는 총 전류는 $$x_1 \times w_1 + x_2 \times w_2$$ 입니다. 만약 이 전류의 합이 전구가 켜지기 위한 최소 전류의 세기 ($$\theta$$)를 만족하면 전구가 켜지고, 만족하지 못하면 전구는 꺼집니다.


## 논리 회로를 퍼셉트론으로

> 이번에는 간단한 논리 회로를 퍼셉트론으로 표현 해보겠습니다.

### AND 게이트의 진리표

| $$x_1$$ | $$x_2$$ | $$y$$|
| 0 |0              |0     |
| 0 |1              |0     |
| 1 |0              |0     |
| 1 |1              |1     |

퍼셉트론으로 나타내기 위해서는 $$w_1$$, $$w_2$$, $$\theta$$의 값을 진리표의 조건에 맞게 정해야됩니다.

($$w_1$$, $$w_2$$, $$\theta$$)가 어떤 조합이면 다음과 같은 조건을 모두 만족할 수 있을까요?

$$
\begin{cases}
0 \times w_1 + 0 \times w_2 \leq \theta \\
0 \times w_1 + 1 \times w_2 \leq \theta \\
1 \times w_1 + 0 \times w_2 \leq \theta \\
1 \times w_1 + 1 \times w_2 > \theta
\end{cases}
$$

만족하는 매개변수는 무수히 많습니다. (0.5, 0.5, 0.7), (0.5, 0.5, 0.8), (1.0, 1.0, 1.0)일 때 모두 진리표 y값을 만족합니다.

(0.5, 0.5, 0.7)로 예를 들어:

$$
\begin{cases}
0 \times 0.5 + 0 \times 0.5 \leq 0.7 \\
0 \times 0.5 + 1 \times 0.5 \leq 0.7 \\
1 \times 0.5 + 0 \times 0.5 \leq 0.7
\end{cases}
$$

이러한 조건들은 결국엔 `임계점보다 낮아` $$y$$값이 `0`이 되며

$$1 \times 0.5 + 1 \times 0.5 > 0.7$$

마지막 조건에는 `임계점보다 높아` $$y$$값이 `1`이 되게 됩니다. 따라서 AND게이트 진리표대로 값이 나오는 것을 확인할 수 있습니다.

> OR 게이트의 진리표도 한 번 살펴보겠습니다.

### OR 게이트의 진리표

| $$x_1$$ | $$x_2$$ | $$y$$|
| 0 |0              |0     |
| 0 |1              |1     |
| 1 |0              |1     |
| 1 |1              |1     |

퍼셉트론으로 나타내기 위해서는 $$w_1$$, $$w_2$$, $$\theta$$의 값을 진리표의 조건에 맞게 정해야됩니다.

($$w_1$$, $$w_2$$, $$\theta$$)가 어떤 조합이면 다음과 같은 조건을 모두 만족할 수 있을까요?

$$
\begin{cases}
0 \times w_1 + 0 \times w_2 \leq \theta \\
0 \times w_1 + 1 \times w_2 > \theta \\
1 \times w_1 + 0 \times w_2 > \theta \\
1 \times w_1 + 1 \times w_2 > \theta
\end{cases}
$$

만족하는 매개변수는 무수히 많습니다. (0.5, 0.5, 0.4), (0.5, 0.5, 0.3), (1.0, 1.0, 0.0)일 때 모두 진리표 y값을 만족합니다.

(0.5, 0.5, 0.4)로 예를 들어:

$$0 \times 0.5 + 0 \times 0.5 \leq 0.4$$

이러한 조건들은 결국엔 `임계점보다 낮아` $$y$$값이 `0`이 되며

$$
\begin{cases}
0 \times 0.5 + 1 \times 0.5 > 0.4 \\
1 \times 0.5 + 0 \times 0.5 > 0.4 \\
1 \times 0.5 + 1 \times 0.5 > 0.4
\end{cases}
$$

마지막 조건에는 `임계점보다 높아` $$y$$값이 `1`이 되게 됩니다. 따라서 OR게이트의 진리표대로 값이 나오는 것을 확인할 수 있습니다.

> 진리표를 보고 퍼셉트론의 매개변수($$w_1$$,$$w_2$$, $$\theta$$)를 적절하게 조절해서 AND, OR 논리 회로를 퍼셉트론으로 표현 해보았습니다. -> AND게이트 : (0.5, 0.5, 0.7), OR게이트 : (0.5, 0.5, 0.4)

> 퍼셉트론의 매개변수를 설정하는 과정은 우리가 `진리표(학습 데이터)`를 보며 직접 값을 결정하는 방식이었습니다. 반면에, 기계학습 문제에서는 컴퓨터가 이러한 `학습 데이터`를 분석하여 자동으로 매개변수의 값이 정하게 됩니다. 이러한 과정을 `학습`이라고 부릅니다.
{: .prompt-tip}

## 가중치와 편향 도입

> 편향의 도입은 앞으로를 생각해서 도입이 되었다고 하는데 향후 학습 후 관련 포스터를 링크 걸어두겠습니다.
{: .prompt-warning}

편향은 단순히 $$\theta$$를 $$-b$$로 치환하면 됩니다. 수식은 다음과 같습니다.

$$
y = 
\begin{cases}
0 & \text{if } b + w_1x_1 + w_2x_2 \leq 0\\
1 & \text{if } b + w_1x_1 + w_2x_2 > 0
\end{cases}
$$

이전 수식에 양변에 $$b$$를 더한 것과 같습니다.

정리하자면 `가중치`는 결과에 미치는 영향(0이 될 지 1이 될 지의 대한 영향력)을 조절하는 데 사용되며, `편향`은 뉴런이 활성화되는 기준(1이 얼마나 쉽게 될 지, 가중치가 아무리 커봤자 편향이 더 크면 뉴런이 활성화되지 않음)을 조절합니다.

## 퍼셉트론의 한계

XOR 게이트는 어떻게 퍼셉트론으로 나타낼 수 있을까요?

아무리 생각해봐도 가중치와 편향의 값을 무엇으로 정해야할 지 모르겠습니다.

시각적으로도 살펴보겠습니다.

![alt text](/assets/img/posts/AI/underbook/chap2/2.png)

AND, OR, NAND 게이트는 직선으로 `빨간색` 점과 `파란색` 점으로 나누어지는 반면에, XOR 게이트는 직선으로 나눌 방법이 없어보입니다.

여기에서 직선의 영역으로 표현한 것을 `선형 영역`, 곡선의 영역으로 표현한 것을 `비선형 영역`이라고 합니다.

> 왜 직선으로 나눠져야하는 지는 추후 학습에서 포스터를 게시해서 링크를 걸어두겠습니다.
{: .prompt-warning}

